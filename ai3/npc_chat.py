import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json

class SimpleNPCChat:
    """ì½˜ì†”ìš© ê°„ë‹¨í•œ NPC ì±„íŒ… ì‹œìŠ¤í…œ"""
    
    def __init__(self, base_model_id, adapter_path, npc_data_path=None):
        self.base_model_id = base_model_id
        self.adapter_path = adapter_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # NPC ê¸°ë³¸ ì •ë³´ (JSON íŒŒì¼ì´ ì—†ì„ ê²½ìš° ì‚¬ìš©)
        # self.npc_info = {
        #     "name": "ì‹  ê¹€ìƒí”„",
        #     "display_name": "ì‹  ê¹€ìƒí”„",
        #     "personality": "ë‹¤ì •í•˜ê³  ì—„ê²©í•œ ì‹ ",
        #     "role": "í”Œë ˆì´ì–´ì˜ ìŠ¤íƒ¯ê³¼ ì§ì—…ì„ ê²°ì •í•˜ëŠ” ì‹ "
        # }
        
        # NPC ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        if npc_data_path and os.path.exists(npc_data_path):
            with open(npc_data_path, 'r', encoding='utf-8') as f:
                npc_data = json.load(f)
                self.npc_info = {
                    "name": npc_data['basic_info']['name'],
                    "display_name": npc_data['basic_info']['display_name'],
                    "personality": ', '.join(npc_data['personality']['primary_traits']),
                    "role": npc_data['role_and_function']['primary_role']
                }
        
        self.model = None
        self.tokenizer = None
        self.conversation_history = []
        
        self.load_model()
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_id,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            self.model = PeftModel.from_pretrained(base_model, self.adapter_path)
            self.model.eval()
            
            print(f"âœ… {self.npc_info['display_name']} ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return False
        
        return True
    
    def generate_response(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ NPC ì‘ë‹µ ìƒì„±"""
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = f"""ë‹¹ì‹ ì€ ê²Œì„ 'ì–´ë“œë²¤ì²˜ ì›”ë“œ'ì˜ NPC '{self.npc_info['display_name']}'ì…ë‹ˆë‹¤.
ì„±ê²©: {self.npc_info['personality']}
ì—­í• : {self.npc_info['role']}

ì´ ìºë¦­í„°ë¡œì„œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì£¼ì„¸ìš”. ì •ì¤‘í•œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , í”Œë ˆì´ì–´ë¥¼ 'ìš©ì‚¬ë‹˜'ì´ë¼ê³  ë¶€ë¥´ì„¸ìš”."""
        
        # ëŒ€í™” í˜•ì‹ êµ¬ì„±
        conversation = f"<|system|>\n{system_prompt}\n"
        
        # ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        for history in self.conversation_history[-3:]:
            conversation += f"<|user|>\n{history['user']}\n<|assistant|>\n{history['assistant']}\n"
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        conversation += f"<|user|>\n{user_input}\n<|assistant|>\n"
        
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer.encode(conversation, return_tensors="pt").to(self.device)
            
            # ì‘ë‹µ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=200,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            response = full_response[len(conversation):].strip()
            
            # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
            if "<|endoftext|>" in response:
                response = response.split("<|endoftext|>")[0].strip()
            if "<|user|>" in response:
                response = response.split("<|user|>")[0].strip()
            
            return response
            
        except Exception as e:
            print(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def chat(self, user_input):
        """ëŒ€í™” ì²˜ë¦¬"""
        response = self.generate_response(user_input)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.conversation_history.append({
            "user": user_input,
            "assistant": response
        })
        
        return response
    
    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ğŸ’­ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì„¤ì •
    base_model_id = "Bllossom/llama-3.2-Korean-Bllossom-3B"
    adapter_path = "workspace/npc-god-adapter"
    npc_data_path = "data/npcs/npc_god.json"  # ì—†ì–´ë„ ë™ì‘í•¨
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(adapter_path):
        print(f"âŒ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_path}")
        print("ë¨¼ì € fine_tune_npc.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•´ì£¼ì„¸ìš”.")
        return
    
    print("ğŸ® ì–´ë“œë²¤ì²˜ ì›”ë“œ - NPC ì±„íŒ… ì‹œìŠ¤í…œ")
    print("="*50)
    
    # NPC ì±„íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    npc_chat = SimpleNPCChat(base_model_id, adapter_path, npc_data_path)
    
    if npc_chat.model is None:
        return
    
    print("\nëª…ë ¹ì–´:")
    print("- 'quit' ë˜ëŠ” 'exit': ì¢…ë£Œ")
    print("- 'reset': ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”") 
    print("- 'help': ë„ì›€ë§")
    print("- ê·¸ ì™¸: NPCì™€ ëŒ€í™”")
    print("="*50)
    
    # í™˜ì˜ ë©”ì‹œì§€
    welcome_msg = npc_chat.chat("ì•ˆë…•í•˜ì„¸ìš”")
    print(f"\nâ­ {npc_chat.npc_info['display_name']}: {welcome_msg}\n")
    
    # ëŒ€í™” ë£¨í”„
    while True:
        try:
            user_input = input("ğŸ‘¤ ë‹¹ì‹ : ").strip()
            
            # ì¢…ë£Œ ëª…ë ¹
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print(f"\nâ­ {npc_chat.npc_info['display_name']}: ì•ˆë…•íˆ ê°€ì„¸ìš”, ìš©ì‚¬ë‹˜!")
                break
            
            # ì´ˆê¸°í™” ëª…ë ¹
            if user_input.lower() in ['reset', 'ì´ˆê¸°í™”']:
                npc_chat.reset_conversation()
                continue
            
            # ë„ì›€ë§
            if user_input.lower() in ['help', 'ë„ì›€ë§']:
                print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
                print("- quit/exit: í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
                print("- reset: ëŒ€í™” ê¸°ë¡ ì‚­ì œ")
                print("- help: ì´ ë„ì›€ë§ í‘œì‹œ")
                print("- ì¼ë°˜ í…ìŠ¤íŠ¸: NPCì™€ ëŒ€í™”\n")
                continue
            
            # ë¹ˆ ì…ë ¥ ë¬´ì‹œ
            if not user_input:
                continue
            
            # NPC ì‘ë‹µ ìƒì„±
            print("ğŸ¤” (ìƒê° ì¤‘...)", end="", flush=True)
            response = npc_chat.chat(user_input)
            print(f"\râ­ {npc_chat.npc_info['display_name']}: {response}\n")
            
        except KeyboardInterrupt:
            print(f"\n\nâ­ {npc_chat.npc_info['display_name']}: ê°‘ì‘ìŠ¤ëŸ½ê²Œ ë– ë‚˜ì‹œëŠ”êµ°ìš”. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")

if __name__ == "__main__":
    main()