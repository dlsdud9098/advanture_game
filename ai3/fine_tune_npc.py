import os
import json
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model

# í™˜ê²½ ì„¤ì •
os.environ["WANDB_DISABLED"] = "true"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

class NPCDataset:
    """NPC í›ˆë ¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, training_data_path, tokenizer, max_length=1024):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # JSON íŒŒì¼ì—ì„œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        with open(training_data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"âœ… {len(self.data)}ê°œì˜ í›ˆë ¨ ìƒ˜í”Œì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        self.texts = [item["text"] for item in self.data]
    
    def get_dataset(self):
        """Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
        def tokenize_function(examples):
            # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors="pt"
            )
            
            # labels = input_ids (causal LM)
            tokenized["labels"] = tokenized["input_ids"].clone()
            
            return tokenized
        
        # Dataset ìƒì„±
        dataset = Dataset.from_dict({"text": self.texts})
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset

def setup_model_and_tokenizer(model_id, base_path):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
    
    # 8-bit quantization ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_enable_fp32_cpu_offload=True
    )
    
    # ìºì‹œ ë””ë ‰í† ë¦¬
    cache_dir = os.path.join(base_path, 'workspace/cache')
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map='auto',
        cache_dir=cache_dir,
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_id,
        cache_dir=cache_dir,
        trust_remote_code=True
    )
    
    # pad_token ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    return model, tokenizer

def main():
    print("ğŸš€ NPC ë°ì´í„°ë¥¼ í™œìš©í•œ íŒŒì¸íŠœë‹ ì‹œì‘!\n")
    
    # ê¸°ë³¸ ì„¤ì •
    base_path = '/home/apic/python/advanture_game'
    model_id = "Bllossom/llama-3.2-Korean-Bllossom-3B"
    training_data_path = "data/training/god_npc_training.json"
    
    # í›ˆë ¨ ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(training_data_path):
        print(f"âŒ í›ˆë ¨ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {training_data_path}")
        print("ë¨¼ì € NPC ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:")
        print("python save_npc_data.py")
        return
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    
    print("ğŸ¤– ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    model, tokenizer = setup_model_and_tokenizer(model_id, base_path)
    
    print("ğŸ“š NPC í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    npc_dataset = NPCDataset(training_data_path, tokenizer)
    train_dataset = npc_dataset.get_dataset()
    
    print(f"âœ… í›ˆë ¨ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(train_dataset)}ê°œ ìƒ˜í”Œ")
    
    print("ğŸ”§ LoRA ì„¤ì • ì¤‘...")
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    print("âš™ï¸ í›ˆë ¨ ì„¤ì • ì¤‘...")
    output_dir = os.path.join(base_path, 'workspace/npc-god-adapter')
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=50,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=5,
        save_steps=100,
        save_total_limit=3,
        prediction_loss_only=True,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        report_to="none"
    )
    
    # ë°ì´í„° ì½œë ˆì´í„°
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    print("ğŸ¯ Trainer ì„¤ì • ì¤‘...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    print("ğŸš‚ íŒŒì¸íŠœë‹ ì‹œì‘!")
    try:
        # í›ˆë ¨ ì‹¤í–‰
        trainer.train()
        
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ì‹  ê¹€ìƒí”„ í…ŒìŠ¤íŠ¸:")
        test_prompt = "<|system|>\në‹¹ì‹ ì€ ê²Œì„ 'ì–´ë“œë²¤ì²˜ ì›”ë“œ'ì˜ NPC 'ì‹  ê¹€ìƒí”„'ì…ë‹ˆë‹¤.\n<|user|>\nê²Œì„ì„ ì‹œì‘í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.\n<|assistant|>\n"
        
        inputs = tokenizer.encode(test_prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("ì‘ë‹µ:", response[len(test_prompt):])
        
        print(f"\nğŸ‰ NPC íŒŒì¸íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()